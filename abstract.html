---
layout: default
title: Abstract
---
	<h2>Distributed Automated Knowledge Acquisition</h2>
	<div class='row'>
		<div class='col-md-4'>
			<h3>Team Members</h3>
			<ul>
				<li>Christopher Harvey, Computer Science</li>
				<li>Sergey Matskevich, Computer Science</li>
				<li>Ernesto Cejas Padilla, Computer Science</li>
				<li>ByeongGil Jeon, Computer Science</li>
				<li>Jirakit Songprasit, Computer Science</li>
			</ul>
		</div>
		<div class='col-md-4'>
			<h3>Advisors</h3>
			<ul>
				<li>Bill Mongan, Computer Science</li>
				<li>Jeff Popyack, Computer Science</li>
			</ul>
		</div>
		<div class='col-md-4'>
			<h3>Stakeholders</h3>
			<ul>
				<li>Brian Seidman, iPipeline</li>
				<li>Nou Chounlamountry, iPipeline</li>
			</ul>
		</div>
	</div>
	<div class='row'>
		<div class='col-md-8'>
			<p>
				The Distributed Automated Knowledge Acquisition (DAKA) project is a business intelligence tool that will use BigData techniques to extract knowledge from large data sets. DAKA will run data mining algorithms on a variety of large data sets. The Hadoop framework will distribute the computation and MongoDB will distribute the storage across several machines. The scalability of these tools will allow DAKA to handle problems of different sizes. DAKA’s primary use case will be to find insurance sales opportunities by analyzing public data sets. This use case will demonstrate DAKA’s ability to store and process up to one hundred terabytes of data and produce actionable intelligence.
			</p>
		</div>
		<div class='col-md-4'>
			<img src='Logo.png' />
		</div>
	</div>

